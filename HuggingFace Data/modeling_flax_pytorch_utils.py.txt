PyTorch - Flax general utilities.
Load pytorch checkpoints in a flax model
Rename PT weight names to corresponding Flax weight names and reshape tensor if necessary
Checks if `key` of `(prefix,) + key` is in random_flax_state_dict
Load flax checkpoints in a PyTorch model
Load flax checkpoints in a PyTorch model
# coding=utf-8
# Copyright 2021 The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#####################
# PyTorch => Flax #
#####################
# noqa: F401
# noqa: F401
# model is sharded and pytorch_checkpoint_path already contains the list of .pt shard files
# layer norm
# batch norm layer mean
# batch norm layer var
# embedding
# conv layer
# linear layer
# old PyTorch layer norm weight
# old PyTorch layer norm bias
# New `weight_norm` from https://github.com/huggingface/transformers/pull/24030
# convert pytorch tensor to numpy
# numpy currently does not support bfloat16, need to go over float32 in this case to not lose precision
# use params dict if the model contains batch norm layers
# add batch_stats keys,values to dict
# Need to change some parameters name to match Flax names
# remove base model prefix if necessary
# Correctly rename weight parameters
# add model prefix if necessary
# add batch stats if the model contains batchnorm layers
# remove num_batches_tracked key
# also add unexpected weight so that warning is thrown
# also add unexpected weight so that warning is thrown
############################
# Sharded Pytorch => Flax #
############################
# Load the index
# load using msgpack utils
# use params dict if the model contains batch norm layers and then add batch_stats keys,values to dict
# Need to change some parameters name to match Flax names
# remove base model prefix if necessary
# Correctly rename weight parameters
# add model prefix if necessary
# add batch stats if the model contains batchnorm layers
# remove num_batches_tracked key
# also add unexpected weight so that warning is thrown
# also add unexpected weight so that warning is thrown
#####################
# Flax => PyTorch #
#####################
# import correct flax class
# load flax weight dict
# noqa: F401
# check if we have bf16 weights
# convert all weights to fp32 if the are bf16 since torch.from_numpy can-not handle bf16
# and bf16 is not fully supported in PT yet.
# keep track of unexpected & missing keys
# adapt flax_key to prepare for loading from/to base model only
# rename flax weights to PyTorch format
# conv layer
# linear layer
# adding batch stats from flax batch norm to pt
# Remove the params/batch_stats header
# We also need to look at `pt_model_dict` and see if there are keys requiring further transformation.
# New `weight_norm` from https://github.com/huggingface/transformers/pull/24030
# add weight to pytorch dict
# remove from missing keys
# weight is not expected by PyTorch model
# re-transform missing_keys to list