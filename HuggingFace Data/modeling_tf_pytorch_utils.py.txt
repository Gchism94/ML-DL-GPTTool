PyTorch - TF 2.0 general utilities.

    Possible ...
    

    Convert a TF 2.0 model variable name in a pytorch model weight name.

    Conventions for TF2.0 scopes -> PyTorch attribute names conversions:

        - '$1___$2' is replaced by $2 (can be used to duplicate or remove layers in TF2.0 vs PyTorch)
        - '_._' is replaced by a new level separation (can be used to convert TF2.0 lists in PyTorch nn.ModulesList)

    return tuple with:

        - pytorch model weight name
        - transpose: `TransposeType` member indicating whether and how TF2.0 and PyTorch weights matrices should be
          transposed with regards to each other
    

    Apply a transpose to some weight then tries to reshape the weight to the same shape as a given shape, all in a
    framework agnostic way.
    
Load pytorch checkpoints in a TF 2.0 model
Load pytorch checkpoints in a TF 2.0 model
Load pytorch state_dict in a TF 2.0 model.
Load a pytorch state_dict in a TF 2.0 model. pt_state_dict can be either an actual dict or a lazy-loading
    safetensors archive created with the safe_open() function.

    Load TF 2.0 HDF5 checkpoint in a PyTorch model We use HDF5 to easily do transfer learning (see
    https://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/python/keras/engine/network.py#L1352-L1357).
    
Load TF 2.0 model in a pytorch model
Load TF2.0 symbolic weights in a PyTorch model
# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# device ids
# '$1___$2' is replaced by $2 (can be used to duplicate or remove layers in TF2.0 vs PyTorch)
# '_._' is replaced by a level separation (can be used to convert TF2.0 lists in PyTorch nn.ModulesList)
# Remove empty levels at the end
# Convert from TF2.0 '/' separators to PyTorch '.' separators
# Some weights have a single name without "/" such as final_logits_bias in BART
# Remove level zero
# When should we transpose the weights
# Convert standard TF2.0 names in PyTorch names
# The SeparableConv1D TF layer contains two weights that are translated to PyTorch Conv1D here
# Remove prefix if needed
# Conv2D weight:
#    PT: (num_out_channel, num_in_channel, kernel[0], kernel[1])
# -> TF: (kernel[0], kernel[1], num_in_channel, num_out_channel)
# Conv1D weight:
#    PT: (num_out_channel, num_in_channel, kernel)
# -> TF: (kernel, num_in_channel, num_out_channel)
#####################
# PyTorch => TF 2.0 #
#####################
# noqa: F401
# noqa: F401
# noqa: F401
# noqa: F401
# Treats a single file as a collection of shards with 1 shard.
# Loads all shards into a single state dictionary
# noqa: F401
# noqa: F401
# Numpy doesn't understand bfloat16, so upcast to a dtype that doesn't lose precision
# Make sure model is built
# Convert old format to new format if needed from a PyTorch state_dict
# New `weight_norm` from https://github.com/huggingface/transformers/pull/24030
# Matt: All TF models store the actual model stem in a MainLayer class, including the base model.
# In PT, the derived models (with heads) use the base model class as the stem instead,
# and there is no MainLayer class. This means that TF base classes have one
# extra layer in their weight names, corresponding to the MainLayer class. This code block compensates for that.
# Is a tuple to account for possible name aliasing
# The aliases are in priority order, take the first one that matches
# If none of the aliases match, just use the first one (it'll be reported as missing)
# Find associated numpy array in pytorch model state dict
# authorized missing keys don't have to be loaded
# Immediately free memory to keep peak usage as low as possible
# We will emit merged warnings at the end
# Now we just need to merge the loading info
# Keys are missing only if they're missing in *every* shard
# Keys are unexpected/mismatched if they're unexpected/mismatched in *any* shard
#####################
# TF 2.0 => PyTorch #
#####################
#L1352-L1357).
# noqa: F401
# noqa: F401
# Instantiate and load the associated TF 2.0 model
# Add "TF" at the beginning
# Make sure model is built
# noqa: F401
# noqa: F401
# Make sure we are able to load PyTorch base models as well as derived models (with heads)
# TF models always have a prefix, some of PyTorch models (base ones) don't
# Build a map from potential PyTorch weight names to TF 2.0 Variables
# Handle PyTorch shared weight ()not duplicated in TF 2.0
# New `weight_norm` from https://github.com/huggingface/transformers/pull/24030
# Find associated numpy array in pytorch model state dict
# Convert to torch tensor
# Some models may have keys that are not in the state by design, removing them before needlessly warning
# the user.