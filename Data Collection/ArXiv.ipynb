{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries. \n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the file\n",
    "\n",
    "def clean_latex(text):\n",
    "    # Remove LaTeX math expressions\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)  # Inline math ($...$)\n",
    "    text = re.sub(r'\\\\begin{.*?}.*?\\\\end{.*?}', '', text, flags=re.DOTALL)  # Math environments (e.g., equation)\n",
    "    \n",
    "    # Remove LaTeX commands (like \\cite, \\textbf, \\section, etc.)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+{.*?}', '', text)  # Commands like \\textbf{}\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\s*', '', text)  # Standalone commands like \\section\n",
    "    \n",
    "    # Remove curly braces used in LaTeX for grouping\n",
    "    text = text.replace('{', '').replace('}', '')\n",
    "    \n",
    "    # Remove citations (e.g., \\cite{some_ref})\n",
    "    text = re.sub(r'\\\\cite{.*?}', '', text)\n",
    "\n",
    "    # Remove figure/table references (e.g., Figure 1, Table 2)\n",
    "    text = re.sub(r'(Figure|Table) \\d+', '', text)\n",
    "\n",
    "    # Remove excessive technical terms related to data or experiments that are irrelevant\n",
    "    text = re.sub(r'\\d+\\s+samples|\\d+\\s+epochs|training\\s+set|validation\\s+set|test\\s+set', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ArXiv API endpoint and query parameters\n",
    "# https://arxiv.org/category_taxonomy Link \n",
    "\n",
    "url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    'search_query': '(cat:cs.LG)',\n",
    "    'start': 0,\n",
    "    'max_results': 200,  # Increase the number of papers\n",
    "    'sortBy': 'relevance',\n",
    "    'sortOrder': 'descending'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request to ArXiv API\n",
    "response = requests.get(url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the response\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "# Function to extract useful information from each paper\n",
    "\n",
    "def parse_papers(root):\n",
    "    papers = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "        papers.append({\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'link': link\n",
    "        })\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'model selection', 'cross-validation', 'hyperparameter tuning', 'grid search',\n",
    "    'Bayesian optimization', 'train-test split', 'performance metrics', 'k-fold cross-validation',\n",
    "    'leave-one-out cross-validation', 'regularization', 'L1 regularization', 'L2 regularization',\n",
    "    'AUC-ROC', 'hyperparameter optimization', 'early stopping', 'overfitting prevention',\n",
    "    'bias-variance tradeoff', 'dropout', 'weight decay'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter papers by keywords\n",
    "\n",
    "def filter_papers_by_keywords(papers, keywords):\n",
    "    filtered_papers = []\n",
    "    for paper in papers:\n",
    "        if any(keyword.lower() in paper['summary'].lower() for keyword in keywords):\n",
    "            filtered_papers.append(paper)\n",
    "    return filtered_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "# Get the papers and filter them by keywords\n",
    "\n",
    "papers = parse_papers(root)\n",
    "filtered_papers = filter_papers_by_keywords(papers, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in filtered_papers:\n",
    "    paper['cleaned_summary'] = clean_latex(paper['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Paper 1: Efficient algorithms for decision tree cross-validation\n",
      "Original Summary:   Cross-validation is a useful and generally applicable technique often\n",
      "employed in machine learning, including decision tree induction. An important\n",
      "disadvantage of straightforward implementation of the technique is its\n",
      "computational overhead. In this paper we show that, for decision trees, the\n",
      "computational overhead of cross-validation can be reduced significantly by\n",
      "integrating the cross-validation with the normal decision tree induction\n",
      "process. We discuss how existing decision tree algorithms can be adapted to\n",
      "this aim, and provide an analysis of the speedups these adaptations may yield.\n",
      "The analysis is supported by experimental results.\n",
      "\n",
      "Cleaned Summary:   Cross-validation is a useful and generally applicable technique often\n",
      "employed in machine learning, including decision tree induction. An important\n",
      "disadvantage of straightforward implementation of the technique is its\n",
      "computational overhead. In this paper we show that, for decision trees, the\n",
      "computational overhead of cross-validation can be reduced significantly by\n",
      "integrating the cross-validation with the normal decision tree induction\n",
      "process. We discuss how existing decision tree algorithms can be adapted to\n",
      "this aim, and provide an analysis of the speedups these adaptations may yield.\n",
      "The analysis is supported by experimental results.\n",
      "\n",
      "Link: http://arxiv.org/abs/cs/0110036v1\n",
      "\n",
      "Filtered Paper 2: Stability Analysis for Regularized Least Squares Regression\n",
      "Original Summary:   We discuss stability for a class of learning algorithms with respect to noisy\n",
      "labels. The algorithms we consider are for regression, and they involve the\n",
      "minimization of regularized risk functionals, such as L(f) := 1/N sum_i\n",
      "(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\n",
      "y_i is a noisy version of f*(x_i) for some function f* in H, the output of the\n",
      "algorithm converges to f* as the regularization term and noise simultaneously\n",
      "vanish. We consider two flavors of this problem, one where a data set of N\n",
      "points remains fixed, and the other where N -> infinity. For the case where N\n",
      "-> infinity, we give conditions for convergence to f_E (the function which is\n",
      "the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\n",
      "describe the limiting 'non-noisy', 'non-regularized' function f*, and give\n",
      "conditions for convergence. In the process, we develop a set of tools for\n",
      "dealing with functionals such as L(f), which are applicable to many other\n",
      "problems in learning theory.\n",
      "\n",
      "Cleaned Summary:   We discuss stability for a class of learning algorithms with respect to noisy\n",
      "labels. The algorithms we consider are for regression, and they involve the\n",
      "minimization of regularized risk functionals, such as L(f) := 1/N sum_i\n",
      "(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\n",
      "y_i is a noisy version of f*(x_i) for some function f* in H, the output of the\n",
      "algorithm converges to f* as the regularization term and noise simultaneously\n",
      "vanish. We consider two flavors of this problem, one where a data set of N\n",
      "points remains fixed, and the other where N -> infinity. For the case where N\n",
      "-> infinity, we give conditions for convergence to f_E (the function which is\n",
      "the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\n",
      "describe the limiting 'non-noisy', 'non-regularized' function f*, and give\n",
      "conditions for convergence. In the process, we develop a set of tools for\n",
      "dealing with functionals such as L(f), which are applicable to many other\n",
      "problems in learning theory.\n",
      "\n",
      "Link: http://arxiv.org/abs/cs/0502016v1\n",
      "\n",
      "Filtered Paper 3: Parametric Learning and Monte Carlo Optimization\n",
      "Original Summary:   This paper uncovers and explores the close relationship between Monte Carlo\n",
      "Optimization of a parametrized integral (MCO), Parametric machine-Learning\n",
      "(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\n",
      "contributions. First, we prove that MCO is mathematically identical to a broad\n",
      "class of PL problems. This identity potentially provides a new application\n",
      "domain for all broadly applicable PL techniques: MCO. Second, we introduce\n",
      "immediate sampling, a new version of the Probability Collectives (PC) algorithm\n",
      "for blackbox optimization. Immediate sampling transforms the original BO\n",
      "problem into an MCO problem. Accordingly, by combining these first two\n",
      "contributions, we can apply all PL techniques to BO. In our third contribution\n",
      "we validate this way of improving BO by demonstrating that cross-validation and\n",
      "bagging improve immediate sampling. Finally, conventional MC and MCO procedures\n",
      "ignore the relationship between the sample point locations and the associated\n",
      "values of the integrand; only the values of the integrand at those locations\n",
      "are considered. We demonstrate that one can exploit the sample location\n",
      "information using PL techniques, for example by forming a fit of the sample\n",
      "locations to the associated values of the integrand. This provides an\n",
      "additional way to apply PL techniques to improve MCO.\n",
      "\n",
      "Cleaned Summary:   This paper uncovers and explores the close relationship between Monte Carlo\n",
      "Optimization of a parametrized integral (MCO), Parametric machine-Learning\n",
      "(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\n",
      "contributions. First, we prove that MCO is mathematically identical to a broad\n",
      "class of PL problems. This identity potentially provides a new application\n",
      "domain for all broadly applicable PL techniques: MCO. Second, we introduce\n",
      "immediate sampling, a new version of the Probability Collectives (PC) algorithm\n",
      "for blackbox optimization. Immediate sampling transforms the original BO\n",
      "problem into an MCO problem. Accordingly, by combining these first two\n",
      "contributions, we can apply all PL techniques to BO. In our third contribution\n",
      "we validate this way of improving BO by demonstrating that cross-validation and\n",
      "bagging improve immediate sampling. Finally, conventional MC and MCO procedures\n",
      "ignore the relationship between the sample point locations and the associated\n",
      "values of the integrand; only the values of the integrand at those locations\n",
      "are considered. We demonstrate that one can exploit the sample location\n",
      "information using PL techniques, for example by forming a fit of the sample\n",
      "locations to the associated values of the integrand. This provides an\n",
      "additional way to apply PL techniques to improve MCO.\n",
      "\n",
      "Link: http://arxiv.org/abs/0704.1274v1\n",
      "\n",
      "Filtered Paper 4: Consistency of the group Lasso and multiple kernel learning\n",
      "Original Summary:   We consider the least-square regression problem with regularization by a\n",
      "block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\n",
      "than one. This problem, referred to as the group Lasso, extends the usual\n",
      "regularization by the 1-norm where all spaces have dimension one, where it is\n",
      "commonly referred to as the Lasso. In this paper, we study the asymptotic model\n",
      "consistency of the group Lasso. We derive necessary and sufficient conditions\n",
      "for the consistency of group Lasso under practical assumptions, such as model\n",
      "misspecification. When the linear predictors and Euclidean norms are replaced\n",
      "by functions and reproducing kernel Hilbert norms, the problem is usually\n",
      "referred to as multiple kernel learning and is commonly used for learning from\n",
      "heterogeneous data sources and for non linear variable selection. Using tools\n",
      "from functional analysis, and in particular covariance operators, we extend the\n",
      "consistency results to this infinite dimensional case and also propose an\n",
      "adaptive scheme to obtain a consistent model estimate, even when the necessary\n",
      "condition required for the non adaptive scheme is not satisfied.\n",
      "\n",
      "Cleaned Summary:   We consider the least-square regression problem with regularization by a\n",
      "block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\n",
      "than one. This problem, referred to as the group Lasso, extends the usual\n",
      "regularization by the 1-norm where all spaces have dimension one, where it is\n",
      "commonly referred to as the Lasso. In this paper, we study the asymptotic model\n",
      "consistency of the group Lasso. We derive necessary and sufficient conditions\n",
      "for the consistency of group Lasso under practical assumptions, such as model\n",
      "misspecification. When the linear predictors and Euclidean norms are replaced\n",
      "by functions and reproducing kernel Hilbert norms, the problem is usually\n",
      "referred to as multiple kernel learning and is commonly used for learning from\n",
      "heterogeneous data sources and for non linear variable selection. Using tools\n",
      "from functional analysis, and in particular covariance operators, we extend the\n",
      "consistency results to this infinite dimensional case and also propose an\n",
      "adaptive scheme to obtain a consistent model estimate, even when the necessary\n",
      "condition required for the non adaptive scheme is not satisfied.\n",
      "\n",
      "Link: http://arxiv.org/abs/0707.3390v2\n",
      "\n",
      "Filtered Paper 5: Consistency of trace norm minimization\n",
      "Original Summary:   Regularization by the sum of singular values, also referred to as the trace\n",
      "norm, is a popular technique for estimating low rank rectangular matrices. In\n",
      "this paper, we extend some of the consistency results of the Lasso to provide\n",
      "necessary and sufficient conditions for rank consistency of trace norm\n",
      "minimization with the square loss. We also provide an adaptive version that is\n",
      "rank consistent even when the necessary condition for the non adaptive version\n",
      "is not fulfilled.\n",
      "\n",
      "Cleaned Summary:   Regularization by the sum of singular values, also referred to as the trace\n",
      "norm, is a popular technique for estimating low rank rectangular matrices. In\n",
      "this paper, we extend some of the consistency results of the Lasso to provide\n",
      "necessary and sufficient conditions for rank consistency of trace norm\n",
      "minimization with the square loss. We also provide an adaptive version that is\n",
      "rank consistent even when the necessary condition for the non adaptive version\n",
      "is not fulfilled.\n",
      "\n",
      "Link: http://arxiv.org/abs/0710.2848v1\n",
      "\n",
      "Filtered Paper 6: A New Approach to Collaborative Filtering: Operator Estimation with\n",
      "  Spectral Regularization\n",
      "Original Summary:   We present a general approach for collaborative filtering (CF) using spectral\n",
      "regularization to learn linear operators from \"users\" to the \"objects\" they\n",
      "rate. Recent low-rank type matrix completion approaches to CF are shown to be\n",
      "special cases. However, unlike existing regularization based CF methods, our\n",
      "approach can be used to also incorporate information such as attributes of the\n",
      "users or the objects -- a limitation of existing regularization based CF\n",
      "methods. We then provide novel representer theorems that we use to develop new\n",
      "estimation methods. We provide learning algorithms based on low-rank\n",
      "decompositions, and test them on a standard CF dataset. The experiments\n",
      "indicate the advantages of generalizing the existing regularization based CF\n",
      "methods to incorporate related information about users and objects. Finally, we\n",
      "show that certain multi-task learning methods can be also seen as special cases\n",
      "of our proposed approach.\n",
      "\n",
      "Cleaned Summary:   We present a general approach for collaborative filtering (CF) using spectral\n",
      "regularization to learn linear operators from \"users\" to the \"objects\" they\n",
      "rate. Recent low-rank type matrix completion approaches to CF are shown to be\n",
      "special cases. However, unlike existing regularization based CF methods, our\n",
      "approach can be used to also incorporate information such as attributes of the\n",
      "users or the objects -- a limitation of existing regularization based CF\n",
      "methods. We then provide novel representer theorems that we use to develop new\n",
      "estimation methods. We provide learning algorithms based on low-rank\n",
      "decompositions, and test them on a standard CF dataset. The experiments\n",
      "indicate the advantages of generalizing the existing regularization based CF\n",
      "methods to incorporate related information about users and objects. Finally, we\n",
      "show that certain multi-task learning methods can be also seen as special cases\n",
      "of our proposed approach.\n",
      "\n",
      "Link: http://arxiv.org/abs/0802.1430v2\n",
      "\n",
      "Filtered Paper 7: A Quadratic Loss Multi-Class SVM\n",
      "Original Summary:   Using a support vector machine requires to set two types of hyperparameters:\n",
      "the soft margin parameter C and the parameters of the kernel. To perform this\n",
      "model selection task, the method of choice is cross-validation. Its\n",
      "leave-one-out variant is known to produce an estimator of the generalization\n",
      "error which is almost unbiased. Its major drawback rests in its time\n",
      "requirement. To overcome this difficulty, several upper bounds on the\n",
      "leave-one-out error of the pattern recognition SVM have been derived. Among\n",
      "those bounds, the most popular one is probably the radius-margin bound. It\n",
      "applies to the hard margin pattern recognition SVM, and by extension to the\n",
      "2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\n",
      "as a direct extension of the 2-norm SVM to the multi-class case. For this\n",
      "machine, a generalized radius-margin bound is then established.\n",
      "\n",
      "Cleaned Summary:   Using a support vector machine requires to set two types of hyperparameters:\n",
      "the soft margin parameter C and the parameters of the kernel. To perform this\n",
      "model selection task, the method of choice is cross-validation. Its\n",
      "leave-one-out variant is known to produce an estimator of the generalization\n",
      "error which is almost unbiased. Its major drawback rests in its time\n",
      "requirement. To overcome this difficulty, several upper bounds on the\n",
      "leave-one-out error of the pattern recognition SVM have been derived. Among\n",
      "those bounds, the most popular one is probably the radius-margin bound. It\n",
      "applies to the hard margin pattern recognition SVM, and by extension to the\n",
      "2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\n",
      "as a direct extension of the 2-norm SVM to the multi-class case. For this\n",
      "machine, a generalized radius-margin bound is then established.\n",
      "\n",
      "Link: http://arxiv.org/abs/0804.4898v1\n",
      "\n",
      "Filtered Paper 8: Graph Kernels\n",
      "Original Summary:   We present a unified framework to study graph kernels, special cases of which\n",
      "include the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05},\n",
      "marginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},\n",
      "and geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear\n",
      "algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\n",
      "Sylvester equation, we construct an algorithm that improves the time complexity\n",
      "of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,\n",
      "conjugate gradient solvers or fixed-point iterations bring our algorithm into\n",
      "the sub-cubic domain. Experiments on graphs from bioinformatics and other\n",
      "application domains show that it is often more than a thousand times faster\n",
      "than previous approaches. We then explore connections between diffusion kernels\n",
      "\\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels,\n",
      "and use these connections to propose new graph kernels. Finally, we show that\n",
      "rational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized\n",
      "to graphs reduce to the random walk graph kernel.\n",
      "\n",
      "Cleaned Summary:   We present a unified framework to study graph kernels, special cases of which\n",
      "include the random walk graph kernel ,\n",
      "marginalized graph kernel ,\n",
      "and geometric kernel on graphs . Through extensions of linear\n",
      "algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\n",
      "Sylvester equation, we construct an algorithm that improves the time complexity\n",
      "of kernel computation from  to . When the graphs are sparse,\n",
      "conjugate gradient solvers or fixed-point iterations bring our algorithm into\n",
      "the sub-cubic domain. Experiments on graphs from bioinformatics and other\n",
      "application domains show that it is often more than a thousand times faster\n",
      "than previous approaches. We then explore connections between diffusion kernels\n",
      ", regularization on graphs , and graph kernels,\n",
      "and use these connections to propose new graph kernels. Finally, we show that\n",
      "rational kernels  when specialized\n",
      "to graphs reduce to the random walk graph kernel.\n",
      "\n",
      "Link: http://arxiv.org/abs/0807.0093v1\n",
      "\n",
      "Filtered Paper 9: When is there a representer theorem? Vector versus matrix regularizers\n",
      "Original Summary:   We consider a general class of regularization methods which learn a vector of\n",
      "parameters on the basis of linear measurements. It is well known that if the\n",
      "regularizer is a nondecreasing function of the inner product then the learned\n",
      "vector is a linear combination of the input data. This result, known as the\n",
      "{\\em representer theorem}, is at the basis of kernel-based methods in machine\n",
      "learning. In this paper, we prove the necessity of the above condition, thereby\n",
      "completing the characterization of kernel methods based on regularization. We\n",
      "further extend our analysis to regularization methods which learn a matrix, a\n",
      "problem which is motivated by the application to multi-task learning. In this\n",
      "context, we study a more general representer theorem, which holds for a larger\n",
      "class of regularizers. We provide a necessary and sufficient condition for\n",
      "these class of matrix regularizers and highlight them with some concrete\n",
      "examples of practical importance. Our analysis uses basic principles from\n",
      "matrix theory, especially the useful notion of matrix nondecreasing function.\n",
      "\n",
      "Cleaned Summary:   We consider a general class of regularization methods which learn a vector of\n",
      "parameters on the basis of linear measurements. It is well known that if the\n",
      "regularizer is a nondecreasing function of the inner product then the learned\n",
      "vector is a linear combination of the input data. This result, known as the\n",
      "representer theorem, is at the basis of kernel-based methods in machine\n",
      "learning. In this paper, we prove the necessity of the above condition, thereby\n",
      "completing the characterization of kernel methods based on regularization. We\n",
      "further extend our analysis to regularization methods which learn a matrix, a\n",
      "problem which is motivated by the application to multi-task learning. In this\n",
      "context, we study a more general representer theorem, which holds for a larger\n",
      "class of regularizers. We provide a necessary and sufficient condition for\n",
      "these class of matrix regularizers and highlight them with some concrete\n",
      "examples of practical importance. Our analysis uses basic principles from\n",
      "matrix theory, especially the useful notion of matrix nondecreasing function.\n",
      "\n",
      "Link: http://arxiv.org/abs/0809.1590v1\n",
      "\n",
      "Filtered Paper 10: Stability Bound for Stationary Phi-mixing and Beta-mixing Processes\n",
      "Original Summary:   Most generalization bounds in learning theory are based on some measure of\n",
      "the complexity of the hypothesis class used, independently of any algorithm. In\n",
      "contrast, the notion of algorithmic stability can be used to derive tight\n",
      "generalization bounds that are tailored to specific learning algorithms by\n",
      "exploiting their particular properties. However, as in much of learning theory,\n",
      "existing stability analyses and bounds apply only in the scenario where the\n",
      "samples are independently and identically distributed. In many machine learning\n",
      "applications, however, this assumption does not hold. The observations received\n",
      "by the learning algorithm often have some inherent temporal dependence.\n",
      "  This paper studies the scenario where the observations are drawn from a\n",
      "stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\n",
      "the study of non-i.i.d. processes that implies a dependence between\n",
      "observations weakening over time. We prove novel and distinct stability-based\n",
      "generalization bounds for stationary phi-mixing and beta-mixing sequences.\n",
      "These bounds strictly generalize the bounds given in the i.i.d. case and apply\n",
      "to all stable learning algorithms, thereby extending the use of\n",
      "stability-bounds to non-i.i.d. scenarios.\n",
      "  We also illustrate the application of our phi-mixing generalization bounds to\n",
      "general classes of learning algorithms, including Support Vector Regression,\n",
      "Kernel Ridge Regression, and Support Vector Machines, and many other kernel\n",
      "regularization-based and relative entropy-based regularization algorithms.\n",
      "These novel bounds can thus be viewed as the first theoretical basis for the\n",
      "use of these algorithms in non-i.i.d. scenarios.\n",
      "\n",
      "Cleaned Summary:   Most generalization bounds in learning theory are based on some measure of\n",
      "the complexity of the hypothesis class used, independently of any algorithm. In\n",
      "contrast, the notion of algorithmic stability can be used to derive tight\n",
      "generalization bounds that are tailored to specific learning algorithms by\n",
      "exploiting their particular properties. However, as in much of learning theory,\n",
      "existing stability analyses and bounds apply only in the scenario where the\n",
      "samples are independently and identically distributed. In many machine learning\n",
      "applications, however, this assumption does not hold. The observations received\n",
      "by the learning algorithm often have some inherent temporal dependence.\n",
      "  This paper studies the scenario where the observations are drawn from a\n",
      "stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\n",
      "the study of non-i.i.d. processes that implies a dependence between\n",
      "observations weakening over time. We prove novel and distinct stability-based\n",
      "generalization bounds for stationary phi-mixing and beta-mixing sequences.\n",
      "These bounds strictly generalize the bounds given in the i.i.d. case and apply\n",
      "to all stable learning algorithms, thereby extending the use of\n",
      "stability-bounds to non-i.i.d. scenarios.\n",
      "  We also illustrate the application of our phi-mixing generalization bounds to\n",
      "general classes of learning algorithms, including Support Vector Regression,\n",
      "Kernel Ridge Regression, and Support Vector Machines, and many other kernel\n",
      "regularization-based and relative entropy-based regularization algorithms.\n",
      "These novel bounds can thus be viewed as the first theoretical basis for the\n",
      "use of these algorithms in non-i.i.d. scenarios.\n",
      "\n",
      "Link: http://arxiv.org/abs/0811.1629v1\n",
      "\n",
      "Filtered Paper 11: Stability Analysis and Learning Bounds for Transductive Regression\n",
      "  Algorithms\n",
      "Original Summary:   This paper uses the notion of algorithmic stability to derive novel\n",
      "generalization bounds for several families of transductive regression\n",
      "algorithms, both by using convexity and closed-form solutions. Our analysis\n",
      "helps compare the stability of these algorithms. It also shows that a number of\n",
      "widely used transductive regression algorithms are in fact unstable. Finally,\n",
      "it reports the results of experiments with local transductive regression\n",
      "demonstrating the benefit of our stability bounds for model selection, for one\n",
      "of the algorithms, in particular for determining the radius of the local\n",
      "neighborhood used by the algorithm.\n",
      "\n",
      "Cleaned Summary:   This paper uses the notion of algorithmic stability to derive novel\n",
      "generalization bounds for several families of transductive regression\n",
      "algorithms, both by using convexity and closed-form solutions. Our analysis\n",
      "helps compare the stability of these algorithms. It also shows that a number of\n",
      "widely used transductive regression algorithms are in fact unstable. Finally,\n",
      "it reports the results of experiments with local transductive regression\n",
      "demonstrating the benefit of our stability bounds for model selection, for one\n",
      "of the algorithms, in particular for determining the radius of the local\n",
      "neighborhood used by the algorithm.\n",
      "\n",
      "Link: http://arxiv.org/abs/0904.0814v1\n",
      "\n",
      "Filtered Paper 12: A Bayesian Model for Supervised Clustering with the Dirichlet Process\n",
      "  Prior\n",
      "Original Summary:   We develop a Bayesian framework for tackling the supervised clustering\n",
      "problem, the generic problem encountered in tasks such as reference matching,\n",
      "coreference resolution, identity uncertainty and record linkage. Our clustering\n",
      "model is based on the Dirichlet process prior, which enables us to define\n",
      "distributions over the countably infinite sets that naturally arise in this\n",
      "problem. We add supervision to our model by positing the existence of a set of\n",
      "unobserved random variables (we call these \"reference types\") that are generic\n",
      "across all clusters. Inference in our framework, which requires integrating\n",
      "over infinitely many parameters, is solved using Markov chain Monte Carlo\n",
      "techniques. We present algorithms for both conjugate and non-conjugate priors.\n",
      "We present a simple--but general--parameterization of our model based on a\n",
      "Gaussian assumption. We evaluate this model on one artificial task and three\n",
      "real-world tasks, comparing it against both unsupervised and state-of-the-art\n",
      "supervised algorithms. Our results show that our model is able to outperform\n",
      "other models across a variety of tasks and performance metrics.\n",
      "\n",
      "Cleaned Summary:   We develop a Bayesian framework for tackling the supervised clustering\n",
      "problem, the generic problem encountered in tasks such as reference matching,\n",
      "coreference resolution, identity uncertainty and record linkage. Our clustering\n",
      "model is based on the Dirichlet process prior, which enables us to define\n",
      "distributions over the countably infinite sets that naturally arise in this\n",
      "problem. We add supervision to our model by positing the existence of a set of\n",
      "unobserved random variables (we call these \"reference types\") that are generic\n",
      "across all clusters. Inference in our framework, which requires integrating\n",
      "over infinitely many parameters, is solved using Markov chain Monte Carlo\n",
      "techniques. We present algorithms for both conjugate and non-conjugate priors.\n",
      "We present a simple--but general--parameterization of our model based on a\n",
      "Gaussian assumption. We evaluate this model on one artificial task and three\n",
      "real-world tasks, comparing it against both unsupervised and state-of-the-art\n",
      "supervised algorithms. Our results show that our model is able to outperform\n",
      "other models across a variety of tasks and performance metrics.\n",
      "\n",
      "Link: http://arxiv.org/abs/0907.0808v1\n",
      "\n",
      "Filtered Paper 13: Collaborative Filtering in a Non-Uniform World: Learning with the\n",
      "  Weighted Trace Norm\n",
      "Original Summary:   We show that matrix completion with trace-norm regularization can be\n",
      "significantly hurt when entries of the matrix are sampled non-uniformly. We\n",
      "introduce a weighted version of the trace-norm regularizer that works well also\n",
      "with non-uniform sampling. Our experimental results demonstrate that the\n",
      "weighted trace-norm regularization indeed yields significant gains on the\n",
      "(highly non-uniformly sampled) Netflix dataset.\n",
      "\n",
      "Cleaned Summary:   We show that matrix completion with trace-norm regularization can be\n",
      "significantly hurt when entries of the matrix are sampled non-uniformly. We\n",
      "introduce a weighted version of the trace-norm regularizer that works well also\n",
      "with non-uniform sampling. Our experimental results demonstrate that the\n",
      "weighted trace-norm regularization indeed yields significant gains on the\n",
      "(highly non-uniformly sampled) Netflix dataset.\n",
      "\n",
      "Link: http://arxiv.org/abs/1002.2780v1\n",
      "\n",
      "Filtered Paper 14: Adaptive Bound Optimization for Online Convex Optimization\n",
      "Original Summary:   We introduce a new online convex optimization algorithm that adaptively\n",
      "chooses its regularization function based on the loss functions observed so\n",
      "far. This is in contrast to previous algorithms that use a fixed regularization\n",
      "function such as L2-squared, and modify it only via a single time-dependent\n",
      "parameter. Our algorithm's regret bounds are worst-case optimal, and for\n",
      "certain realistic classes of loss functions they are much better than existing\n",
      "bounds. These bounds are problem-dependent, which means they can exploit the\n",
      "structure of the actual problem instance. Critically, however, our algorithm\n",
      "does not need to know this structure in advance. Rather, we prove competitive\n",
      "guarantees that show the algorithm provides a bound within a constant factor of\n",
      "the best possible bound (of a certain functional form) in hindsight.\n",
      "\n",
      "Cleaned Summary:   We introduce a new online convex optimization algorithm that adaptively\n",
      "chooses its regularization function based on the loss functions observed so\n",
      "far. This is in contrast to previous algorithms that use a fixed regularization\n",
      "function such as L2-squared, and modify it only via a single time-dependent\n",
      "parameter. Our algorithm's regret bounds are worst-case optimal, and for\n",
      "certain realistic classes of loss functions they are much better than existing\n",
      "bounds. These bounds are problem-dependent, which means they can exploit the\n",
      "structure of the actual problem instance. Critically, however, our algorithm\n",
      "does not need to know this structure in advance. Rather, we prove competitive\n",
      "guarantees that show the algorithm provides a bound within a constant factor of\n",
      "the best possible bound (of a certain functional form) in hindsight.\n",
      "\n",
      "Link: http://arxiv.org/abs/1002.4908v2\n",
      "\n",
      "Filtered Paper 15: Model Selection with the Loss Rank Principle\n",
      "Original Summary:   A key issue in statistics and machine learning is to automatically select the\n",
      "\"right\" model complexity, e.g., the number of neighbors to be averaged over in\n",
      "k nearest neighbor (kNN) regression or the polynomial degree in regression with\n",
      "polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\n",
      "for model selection in regression and classification. It is based on the loss\n",
      "rank, which counts how many other (fictitious) data would be fitted better.\n",
      "LoRP selects the model that has minimal loss rank. Unlike most penalized\n",
      "maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\n",
      "regression functions and the loss function. It works without a stochastic noise\n",
      "model, and is directly applicable to any non-parametric regressor, like kNN.\n",
      "\n",
      "Cleaned Summary:   A key issue in statistics and machine learning is to automatically select the\n",
      "\"right\" model complexity, e.g., the number of neighbors to be averaged over in\n",
      "k nearest neighbor (kNN) regression or the polynomial degree in regression with\n",
      "polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\n",
      "for model selection in regression and classification. It is based on the loss\n",
      "rank, which counts how many other (fictitious) data would be fitted better.\n",
      "LoRP selects the model that has minimal loss rank. Unlike most penalized\n",
      "maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\n",
      "regression functions and the loss function. It works without a stochastic noise\n",
      "model, and is directly applicable to any non-parametric regressor, like kNN.\n",
      "\n",
      "Link: http://arxiv.org/abs/1003.0516v1\n",
      "\n",
      "Filtered Paper 16: Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable\n",
      "  Information Criterion in Singular Learning Theory\n",
      "Original Summary:   In regular statistical models, the leave-one-out cross-validation is\n",
      "asymptotically equivalent to the Akaike information criterion. However, since\n",
      "many learning machines are singular statistical models, the asymptotic behavior\n",
      "of the cross-validation remains unknown. In previous studies, we established\n",
      "the singular learning theory and proposed a widely applicable information\n",
      "criterion, the expectation value of which is asymptotically equal to the\n",
      "average Bayes generalization loss. In the present paper, we theoretically\n",
      "compare the Bayes cross-validation loss and the widely applicable information\n",
      "criterion and prove two theorems. First, the Bayes cross-validation loss is\n",
      "asymptotically equivalent to the widely applicable information criterion as a\n",
      "random variable. Therefore, model selection and hyperparameter optimization\n",
      "using these two values are asymptotically equivalent. Second, the sum of the\n",
      "Bayes generalization error and the Bayes cross-validation error is\n",
      "asymptotically equal to $2\\lambda/n$, where $\\lambda$ is the real log canonical\n",
      "threshold and $n$ is the number of training samples. Therefore the relation\n",
      "between the cross-validation error and the generalization error is determined\n",
      "by the algebraic geometrical structure of a learning machine. We also clarify\n",
      "that the deviance information criteria are different from the Bayes\n",
      "cross-validation and the widely applicable information criterion.\n",
      "\n",
      "Cleaned Summary:   In regular statistical models, the leave-one-out cross-validation is\n",
      "asymptotically equivalent to the Akaike information criterion. However, since\n",
      "many learning machines are singular statistical models, the asymptotic behavior\n",
      "of the cross-validation remains unknown. In previous studies, we established\n",
      "the singular learning theory and proposed a widely applicable information\n",
      "criterion, the expectation value of which is asymptotically equal to the\n",
      "average Bayes generalization loss. In the present paper, we theoretically\n",
      "compare the Bayes cross-validation loss and the widely applicable information\n",
      "criterion and prove two theorems. First, the Bayes cross-validation loss is\n",
      "asymptotically equivalent to the widely applicable information criterion as a\n",
      "random variable. Therefore, model selection and hyperparameter optimization\n",
      "using these two values are asymptotically equivalent. Second, the sum of the\n",
      "Bayes generalization error and the Bayes cross-validation error is\n",
      "asymptotically equal to , where  is the real log canonical\n",
      "threshold and  is the number of training samples. Therefore the relation\n",
      "between the cross-validation error and the generalization error is determined\n",
      "by the algebraic geometrical structure of a learning machine. We also clarify\n",
      "that the deviance information criteria are different from the Bayes\n",
      "cross-validation and the widely applicable information criterion.\n",
      "\n",
      "Link: http://arxiv.org/abs/1004.2316v2\n",
      "\n",
      "Filtered Paper 17: Filtrage vaste marge pour l'étiquetage séquentiel à noyaux de\n",
      "  signaux\n",
      "Original Summary:   We address in this paper the problem of multi-channel signal sequence\n",
      "labeling. In particular, we consider the problem where the signals are\n",
      "contaminated by noise or may present some dephasing with respect to their\n",
      "labels. For that, we propose to jointly learn a SVM sample classifier with a\n",
      "temporal filtering of the channels. This will lead to a large margin filtering\n",
      "that is adapted to the specificity of each channel (noise and time-lag). We\n",
      "derive algorithms to solve the optimization problem and we discuss different\n",
      "filter regularizations for automated scaling or selection of channels. Our\n",
      "approach is tested on a non-linear toy example and on a BCI dataset. Results\n",
      "show that the classification performance on these problems can be improved by\n",
      "learning a large margin filtering.\n",
      "\n",
      "Cleaned Summary:   We address in this paper the problem of multi-channel signal sequence\n",
      "labeling. In particular, we consider the problem where the signals are\n",
      "contaminated by noise or may present some dephasing with respect to their\n",
      "labels. For that, we propose to jointly learn a SVM sample classifier with a\n",
      "temporal filtering of the channels. This will lead to a large margin filtering\n",
      "that is adapted to the specificity of each channel (noise and time-lag). We\n",
      "derive algorithms to solve the optimization problem and we discuss different\n",
      "filter regularizations for automated scaling or selection of channels. Our\n",
      "approach is tested on a non-linear toy example and on a BCI dataset. Results\n",
      "show that the classification performance on these problems can be improved by\n",
      "learning a large margin filtering.\n",
      "\n",
      "Link: http://arxiv.org/abs/1007.0824v1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display filtered papers\n",
    "for idx, paper in enumerate(filtered_papers, 1):\n",
    "    print(f\"Filtered Paper {idx}: {paper['title']}\")\n",
    "    print(f\"Original Summary: {paper['summary']}\")\n",
    "    print(f\"Cleaned Summary: {paper['cleaned_summary']}\")\n",
    "    print(f\"Link: {paper['link']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of research papers in the CSV: 17\n"
     ]
    }
   ],
   "source": [
    "def save_to_csv(papers, filename=\"filtered_papers (ArXiv).csv\"):\n",
    "    # Check if the list is empty\n",
    "    if not papers:\n",
    "        print(\"No filtered papers to save.\")\n",
    "        return\n",
    "    \n",
    "    # Specify the headers for the CSV: Title, Link, and Summary (cleaned_summary is renamed to Summary)\n",
    "    keys = ['title', 'link', 'summary']  # Renaming 'cleaned_summary' to 'summary' in the CSV\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        \n",
    "        # Write each paper's title, link, and cleaned summary\n",
    "        for paper in papers:\n",
    "            dict_writer.writerow({\n",
    "                'title': paper['title'],\n",
    "                'link': paper['link'],\n",
    "                'summary': paper['cleaned_summary']  # Rename cleaned_summary as 'summary' here\n",
    "            })\n",
    "\n",
    "# Save the filtered papers with cleaned summaries\n",
    "save_to_csv(filtered_papers)\n",
    "\n",
    "\n",
    "#  Counting total number of papers in the CSV file.\n",
    "\n",
    "def count_papers(filename=\"filtered_papers (ArXiv).csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    return len(df)\n",
    "\n",
    "# Example usage\n",
    "total_papers = count_papers(\"filtered_papers (ArXiv).csv\")\n",
    "print(f\"Total number of research papers in the CSV: {total_papers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
